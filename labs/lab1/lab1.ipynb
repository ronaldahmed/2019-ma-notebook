{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise consists of two parts:\n",
    "\n",
    "- Part 1: NN from scratch (in-class exercise)\n",
    "- Part 2: Recap Sentiment Analysis system in Sklearn (homework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Neural Network from scratch\n",
    "\n",
    "In this exercise you will implement the forward step of a FFNN from scratch and compare your solution to Keras. It is very important that you understand the basic building blocks (how to encode your instances, the labels, how to do the forward pass and apply the weights, activation function etc). It is essential to understand the basic mechanisms, as that is what underlies many of the deep learning toolkits nowadays and the more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to implement the forward step manually on a small dataset. \n",
    "\n",
    "We are assuming multi-class classification tasks. The labels are $$ y \\in \\{a,b,c\\}$$\n",
    "\n",
    "The input data consists of 4 features each encoding a numeric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "classes = [\"a\",\"b\",\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training data\n",
    "data_train = np.array([\n",
    "                       [5,3,2,2],\n",
    "                       [1,3,4,5],\n",
    "                       [1,2,3,4],\n",
    "                       [0,3,1,5],\n",
    "                       [1,3,1,1]\n",
    "                    ])\n",
    "\n",
    "labels_train = [\"a\", \"b\", \"c\", \"b\", \"b\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Forward pass\n",
    "\n",
    "Implement the forward pass using `numpy` for the feedforward neural network illustrated in the figure.\n",
    "\n",
    "* How many neurons does hidden layer 1 and hidden layer 2 have? Note: the bias note is not shown in the figure, consider them as separate neurons.\n",
    "* How many neurons does the output layer have? And the input layer?\n",
    "* Assume there is a `tanh` activation function between the layers. (hint: you can use `np.tanh`)\n",
    "* Which activation function is on the output layer, given the labels above?\n",
    "* Hint: use `.shape` to check the dimensions of your inputs\n",
    "\n",
    "<img src=\"nn.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the size of the various parts of the feedforward neural network. You can use helper variables such as `input_dim = ?` or `hidden_dim1 = ?` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions to determine sizes\n",
    "batch_size,input_dim = data_train.shape # (bs,inpd)\n",
    "hidden_dim1 = 15\n",
    "hidden_dim2 = 20\n",
    "output_dim = len(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the shape of the parameters to be learned for this network using numpy arrays. For now, simply initialize them with ones or random numbers (`np.ones((3,4))` defines a matrix of ones of size `3x4`, similarly, [np.random.randn](https://www.numpy.org/devdocs/reference/generated/numpy.random.randn.html) `np.random.randn(3,4)` initializes a matrix of the same size with random sample from the standard normal distribution.\n",
    "\n",
    "* What are all the parameters of this neural network and what is their shape?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define all parameters of this NN\n",
    "np.random.seed(42)\n",
    "w1 = np.random.randn(input_dim,hidden_dim1)\n",
    "w2 = np.random.randn(hidden_dim1,hidden_dim2)\n",
    "w3 = np.random.randn(hidden_dim2,output_dim)\n",
    "b1 = np.random.randn(1,hidden_dim1)\n",
    "b2 = np.random.randn(1,hidden_dim2)\n",
    "b3 = np.random.randn(1,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the shape of all parameters, we are ready to \"connect the dots\" and build the network. \n",
    "\n",
    "It is instructive to break the computation of each layer down into two steps: the scores $a1$ are obtained by the linear function followed by the activation applications $\\sigma$ to obtain the representation $z1$, as in:\n",
    "\n",
    "$$ a1 = xW_1 + b_1$$\n",
    "$$ z1 = \\sigma(a1)$$\n",
    "\n",
    "Specify the entire network up to the output layer $z3$, and **up to and exclusive** the final application of the softmax, the last activation function, which is provided.\n",
    "\n",
    "The exact implementation of the softmax might differ from toolkit to toolkit (due to variations in implementation details in order to obtain numerical stability). Therefore, we will use the Keras backend function for the softmax calculation which accesses the tensorflow `Tensor` object. This makes sure that the manual calculations of the forward pass due not differ from the Keras-based implementation just because of the difference in the softmax calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## use this softmax\n",
    "## imports for softmax\n",
    "from keras import backend as K\n",
    "from keras  import activations\n",
    "\n",
    "def keras_softmax(scores):\n",
    "    ## softmax calculation\n",
    "    var = K.variable(value=scores)\n",
    "    act_tf = activations.softmax(var) # returns Tensor\n",
    "    softmax_scores = K.eval(act_tf) # return numpy array\n",
    "    return softmax_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## implement the forward pass (up to and exclusive the softmax) \n",
    "## apply it to the training data `data_train` - use vectorization\n",
    "def forward_pass(input):\n",
    "    z1 = np.tanh(np.dot(input,w1)+b1)\n",
    "    z2 = np.tanh(np.dot(z1,w2)+b2)\n",
    "    z3 = np.dot(z2,w3)+b3\n",
    "    return keras_softmax(z3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_manual = forward_pass(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "## the resulting predictions will be the softmax activations for each output neuron for each training instance\n",
    "print(y_hat_manual.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all predictions sum up to approximately 1 (hint: use `np.sum` with `axis`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 0.99999994, 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_hat_manual, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! you have made it through the manual construction of the forward pass. Now lets check your implementation by comparing it to a set of pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model weights and test weights on evaluation file to check your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to:\n",
    "* load pretrained weights for all parameters\n",
    "* apply the weights to the evaluation data `data_eval`\n",
    "* check that your manual softmax scores match the ones obtained by the pre-trained model `model` that we will load\n",
    "* convert the output to labels and calculate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/weights.pickle\",\"rb\") as f:\n",
    "    weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights you just loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 15)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what do the weights contain?\n",
    "weights[\"W1\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your manual implementation of the forward pass to the evaluation data by using the parameters (weights) you just loaded. This allows you to check if you get the same results back as the model implemented in Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = np.array([\n",
    "                       [1,2,3,5],\n",
    "                       [1,3,1,0],\n",
    "                    ])\n",
    "\n",
    "gold_labels_eval = [\"c\", \"b\"]\n",
    "y_eval = np.array([[0,0,1],[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'data/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-15a9586ca6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load model parameters and model structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# use the model for predicting on the data_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf.gpu-1.13/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf.gpu-1.13/lib/python3.6/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf.gpu-1.13/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf.gpu-1.13/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'data/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('data/model.h5') # load model parameters and model structure\n",
    "\n",
    "# use the model for predicting on the data_eval\n",
    "\n",
    "keras_out = model.evaluate(data_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the weights stored in  `weights` to your manually defined forward pass above. Compare the result to the predictions of the loaded model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.64962824  1.01406278 -0.06164092]\n",
      " [-0.35048741  0.80949804 -0.15451172]]\n",
      "softmax:\n",
      "[[0.04939969 0.70884377 0.24175662]\n",
      " [0.18496649 0.5900222  0.22501126]]\n"
     ]
    }
   ],
   "source": [
    "# load the weights and code up the forward pass manually. Compare to the predictions above.\n",
    "scores = None\n",
    "\n",
    "print(scores)\n",
    "print(\"softmax:\")\n",
    "print(keras_softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the two softmax outputs match your implementation is correct. Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labels to 1-hot representation and evaluate the classifier\n",
    "\n",
    "In this section, we are going to convert the softmax output into actual predicted labels. Then, we evaluate the labels on the output.\n",
    "\n",
    "Many deep learning libraries require one-hot representations for labels, where each dimension corresponds to an output neuron.\n",
    "\n",
    "For example, an instance labeled as 'c' is represent in the one-hot target vector as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vector = [0,0,1] # target vector for class 'c'\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the predicted softmax scores above to the actual predicted label and compare it to the gold standard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'b']\n"
     ]
    }
   ],
   "source": [
    "# implement the conversion of softmax scores to actual labels (a list of labels like [\"a\",\"a\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ['c', 'b']\n",
      "gold: ['c', 'b']\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## calculate accuracy\n",
    "# implement accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Keras has a useful [utility function](https://keras.io/utils/) `to_categorical` to convert categorical data into binary one-hot encodings for training the model. The input to the function has to be numeric, i.e., labels converted to indices (corresponding to the classes, e.g., `2` for `c`).\n",
    "\n",
    "```\n",
    "y_train_one_hot = utils.to_categorical(labels_train_num, num_classes=len(classes))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model yourself in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to build the model that was used to generate the weights above in Keras. Play with the model, change its hyperparameters and observe what happens.\n",
    "\n",
    "* Build the network yourself in Keras (Suggestion: use the functional API)\n",
    "* You will need to convert the training data labels to the one-hot format\n",
    "* Train the network with `SGD` as defined below for 5 epochs. \n",
    "* Modify the hyperparameters of the model (learning rate, number of neurons in the hidden layer, optimizer..) and observe the impact on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 15)                75        \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 20)                320       \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 458\n",
      "Trainable params: 458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Recap sentiment analysis exercise with sklearn\n",
    "\n",
    "Solve the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
